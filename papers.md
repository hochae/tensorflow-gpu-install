# papers
## [23 Deep Learning Papers To Get You Started — Part 1](https://medium.com/@rupak.thakur/23-deep-learning-papers-to-get-you-started-part-1-308f80d7bba2): [[github]](https://github.com/rupak-118/AI-papers.git)
## [What is the best ML paper you read in 2017 and why?](https://www.reddit.com/r/MachineLearning/comments/7n69h0/d_what_is_the_best_ml_paper_you_read_in_2017_and/?sort=confidence)
## [What is the best ML paper you read in 2018 and why?](https://scinapse.io/collections/67774)
1. [NO TIME TO READ AI RESEARCH? WE SUMMARIZED TOP 2018 PAPERS FOR YOU](https://www.topbots.com/most-important-ai-research-papers-2018/)
1. [How 3 engineers built a record-breaking supernova identification system with deep learning:](https://medium.com/@dessa_/space-2-vec-fd900f5566)
1. Dropout: a simple way to prevent neural networks from overfitting: [[paper]](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
2. Dropout: A Simple Way to Prevent Neural Networks from Overfitting: [[paper]](https://www.dropbox.com/s/xfelcfcwzmshedu/dropout.pdf?dl=0)
7. Learning deep features for scene recognition using places database : [[paper]](http://places.csail.mit.edu/places_NIPS14.pdf)
7. [A Review on Multi-Label Learning Algorithms](https://www.computer.org/csdl/journal/tk/2014/08/06471714/13rRUwInvBt)
7. [Scalable Nearest Neighbor Algorithms for High Dimensional Data](https://ieeexplore.ieee.org/document/6809191)
7. Demystifying Pararell and Distributed Deep Learning: An In-Depth Concurrency Analysis: [[paper]](https://arxiv.org/pdf/1802.09941.pdf)
1. [Large-Scale Study of Curiosity-Driven Learning](https://pathak22.github.io/large-scale-curiosity/): [[paper]](https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf)
2. [Runner up: Learning Unsupervised Learning Rules](https://arxiv.org/abs/1804.00222): [[paper]](https://arxiv.org/pdf/1804.00222)
3. [Rethinking statistical learning theory: learning with statistical invariants](https://link.springer.com/article/10.1007/s10994-018-5742-0): [[video]](https://www.youtube.com/watch?v=rNd7PDdhl4c)
4. [Backprop as Functor: A compositional perspective on supervised learning](https://arxiv.org/abs/1711.10455): [[paper]](https://arxiv.org/pdf/1711.10455)
5. [GAN Dissection: Visualizing and Understanding Generative Adversarial Networks](https://arxiv.org/abs/1811.10597): [[paper]](https://arxiv.org/pdf/1811.10597)
6. [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948): [[paper]](https://arxiv.org/pdf/1812.04948)
7. [Noise2Noise: Learning Image Restoration without Clean Data](https://arxiv.org/abs/1803.04189): [[paper]](https://arxiv.org/pdf/1803.04189)
7. [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366): [[paper]](https://arxiv.org/pdf/1806.07366)
7. [An Introduction to Probabilistic Programming](https://arxiv.org/abs/1809.10756): [[paper]](https://arxiv.org/pdf/1809.10756)
7. [IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCU- RACY AND ROBUSTNESS.](https://openreview.net/forum?id=Bygh9j09KX): [[paper]](https://openreview.net/pdf?id=Bygh9j09KX)
7. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805): [[paper]](https://arxiv.org/pdf/1810.04805) / [[video]](https://arxiv.org/pdf/1810.04805)
7. [Phrase-Based & Neural Unsupervised Machine Translation](https://arxiv.org/abs/1804.07755): [[paper]](https://arxiv.org/pdf/1804.07755)
7. [Unsupervised Cipher Cracking Using Discrete GANs](https://arxiv.org/abs/1801.04883): [[paper]](https://arxiv.org/pdf/1801.04883)
7. [Word Translation Without Parallel Data](https://arxiv.org/abs/1710.04087): [[paper]](https://arxiv.org/pdf/1710.04087)
7. [AutoAugment: Learning Augmentation Policies from Data](https://arxiv.org/abs/1805.09501): [[paper]](https://arxiv.org/pdf/1805.09501)
7. [Diversity is All You Need: Learning Skills without a Reward Function](https://arxiv.org/abs/1802.06070): [[paper]](https://arxiv.org/pdf/1802.06070)
7. [Measuring the Intrinsic Dimension of Objective Landscapes](https://arxiv.org/abs/1804.08838): [[paper]](https://arxiv.org/pdf/1804.08838)
7. [NeuroSAT blew me away. RNN learns to solve SAT problems](https://arxiv.org/abs/1802.03685): [[paper]](https://arxiv.org/pdf/1802.03685) / [[video]](https://www.youtube.com/watch?v=EqvzIGY_bI4)
7. [Short text clustering based on Pitman-Yor process mixture model](https://dl.acm.org/citation.cfm?id=3237053)
7. [WESPE: Weakly Supervised Photo Enhancer for Digital Cameras](http://www.vision.ee.ethz.ch/~ihnatova/wespe.html): [[paper]](https://arxiv.org/pdf/1709.01118.pdf)
7. [The 10 coolest papers from CVPR 2018](https://towardsdatascience.com/the-10-coolest-papers-from-cvpr-2018-11cb48585a49)
7. Dynamic Routing Between Capsules: [[paper]](https://arxiv.org/pdf/1710.09829.pdf)
7. [Taskonomy: Disentangling Task Transfer Learning](https://arxiv.org/abs/1804.08328): [[paper]](https://arxiv.org/pdf/1804.08328)
7. [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096): [[paper]](https://arxiv.org/pdf/1809.11096)
7. [Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients](https://arxiv.org/abs/1705.07774): [[paper]](https://arxiv.org/pdf/1705.07774)
7. Large-Scale Evolution of Image Classifiers: [[paper]](https://arxiv.org/pdf/1703.01041.pdf)
7. [BERT – State of the Art Language Model for NLP](https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/)
7. [Support Vector Machines (SVM)](https://old.ixtutor.com/support-vector-machines-svm/)
7. [Deep Bayesian regression models](https://arxiv.org/abs/1806.02160): [[paper]](https://arxiv.org/pdf/1806.02160)
7. [Relational inductive biases, deep learning, and graph networks](https://arxiv.org/abs/1806.01261): [[paper]](https://arxiv.org/pdf/1806.01261)
7. REASONING ABOUT PHYSICAL INTERACTIONS WITH OBJECT-ORIENTED PREDICTION AND PLANNING: [[paper]](https://openreview.net/pdf?id=HJx9EhC9tQ)
7. [Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508): [[paper]](https://arxiv.org/pdf/1808.00508)
7. [IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures](https://arxiv.org/abs/1802.01561): [[paper]](https://arxiv.org/pdf/1802.01561)
7. Predicting Zeros of the Riemann Zeta Function Using Machine Learning: A Comparative Analysis: [[paper]](http://www.sci.sdsu.edu/math-reu/2018-2.pdf)
7. FALKON: An Optimal Large Scale Kernel Method: [[paper]](https://arxiv.org/pdf/1705.10958.pdf)
7. [MACHINE LEARNING ALGORITHMS: 4 TYPES YOU SHOULD KNOW](https://theappsolutions.com/blog/development/machine-learning-algorithm-types/)
7. [Artificial Intelligence Chatbot](https://www.colblog.com/artificial-intelligence-chatbot/)
7. Dank Learning: Generating Memes using DNN: [[paper]](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6909159.pdf)
7. New Algorithms in Machine Learning with Applications in Personalized Medicine: [[paper]](https://dspace.mit.edu/bitstream/handle/1721.1/119284/1065541937-MIT.pdf?sequence=1)
7. [Comparative analysis of discretization methods in Bayesian networks](https://www.sciencedirect.com/science/article/pii/S1364815216308672?via%3Dihub)
7. [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146): [[paper]](https://arxiv.org/pdf/1801.06146)
8. [Approximate Inference for Constructing Astronomical Catalogs from Images](https://arxiv.org/abs/1803.00113): [[paper]](https://arxiv.org/pdf/1803.00113)
8. [Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution](https://arxiv.org/abs/1801.04016): [[paper]](https://arxiv.org/pdf/1801.04016)

